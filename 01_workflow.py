# -*- coding: utf-8 -*-
"""01_workflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-r1dhmJmM2ga-zKpPmOIEzJP7_6WPNzo
"""

# pytorch work flow

what_were_covering = { 1: "data (preparing and load)",
                       2: "build model",
                       3: "fitting the model to data (training)",
                       4: "making predctions and evaluting a model (inference)",
                       5: "saving and loading a model",
                       6: "putting it all together"}

import torch
import numpy as np
from torch import nn # nn contains all if pytorch building blocks for neural networrk
import matplotlib.pyplot as plt

torch.__version__

"""## Data (Preparing and loading)

data can be almost anything in ML

* Excel spreadsheet
* Images
* videos
* Audio
* DNA
* Text


Machine Learning is a game of two parts:
1. Get data into a numerical representation.
2. Build a model to learn a patterns in that numerical representation.
"""

# Creat a know parameter
weight = 0.7
bias = 0.3

# Create
start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim = 1)
Y = weight * X + bias
X[: 10], Y[ :10]

len(X), len(Y)

"""### Splitting data into training and test sets (one of the most important concepts in machine learning in general)"""

# Create a train/test split
train = int(0.8 * len(X))
train

X_train, Y_train = X[: train], Y[: train]
X_test, Y_test = X[train:], Y[train:]
len(X_train), len(Y_train), len(X_test), len(Y_test)

"""visualize data"""

def plot_predicitions(train_data = X_train,
                      train_lables = Y_train,
                      test_data = X_test,
                      test_lables = Y_test,
                      predictions = None):

  """
  Plots training data, test data and compares predictions.
  """
  plt.figure(figsize = (10, 7))

  # Plot training data in Blue
  plt.scatter(train_data, train_lables, c="b", s=4, label="training data")

  # Plot test data in green
  plt.scatter(test_data, test_lables, c="r", s=4, label="testing data")

  # Are there predicitoons?
  if predictions is not None:
      # Plot predictions in red (predictions in test data)
      plt.scatter(test_data, predictions, c="g", s=4, label="predictions")

  # Show the lengend
  plt.legend(prop={"size": 14});

plot_predicitions();

"""## 2. Build Model

What our model does:
* Start with random values (weight & bias)
* Look at training data and adjust the random values to better represent (or get closer to) the ideal values (the weight and bias values we used to creat the data)


How does it do that:
1. Gradient descent
2. Back propagation

"""

class LinearregModel(nn.Module): # <- almost everything in PyTorch inherhits from nn.Module
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1,
                                            requires_grad=True, # gradient
                                            dtype=torch.float))
    self.bias = nn.Parameter(torch.randn(1,
                                         requires_grad=True,
                                         dtype=torch.float))

  # Forward method
  def forward(self, x: torch.Tensor)  -> torch.Tensor:
    return self.weights * x + self.bias # linear regression formula

"""### PyTorch model building esentials

* **torch.nn** -- contains all the buildings blocks for computional graphs(neural network).
* torch.nn.parameter -- What parameters shouild model try and learn, often a Pytorch layer from torch.nn will set these for us.
* torch.nn.Module -- the base class for all neural network modules, if you subclass it, it contains **forward() for the input**.
* torch.optim -- this where the optimizers in Pytorch live, help in gradient decent.
* def forward() -- All nn.Module subclasses required you to overwrite forward(), this method defines what happenes in the forward computation.

### checking the contents of our PyTroch model
"""

# Create a random seed
torch.manual_seed(42)

# Create a instance of the model (subclass of nn.Module)
model_0 = LinearregModel()

# Check the parameters
list(model_0.parameters())

# Lsit names parameters
model_0 .state_dict()

weight , bias

"""### Predicting using `torch.inference_mode()`"""

X_test

with torch.inference_mode(): # prediction = inference  (with out inference it will take lot of time and give some thing diff)
  Y_pred = model_0(X_test)

# same with nograd but above is recommenced
# with torch.no_grad():
#   Y_pred = model_0(X_test)

Y_pred

Y_test

plot_predicitions(predictions=Y_pred)

"""### Training model

one way to measure how poor or how wrong your models predictions are is to use a loss function

*Loss function may be called cost function or criterion in differentareas. For our case we use loss function

Thingds we need to Train:
* **Loss fuction:** A function to measure how wrong your model predictions are to the ideal output.   (**So lower is better**)
* **Optimizer:** Takes into account the loss of a model and adjusts the model's paramater (e.g. weight and bias) list(model_name.parameters()) u cansee the weights and bias in our case to improve the loss function.

And specifically for PyTorch, we need:
* A training loop
* A testing loop
"""

model_0.state_dict()

# set up a loss function
loss_fn = nn.L1Loss()

# setup an Optimizer (Stochastic gradient descent)
optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr = 0.01) # lr = learning rate = possibel the most important hyperparameter you caan set.

"""### Building a training loop(and a testing loop) in the PyTorch

A couple of things we need in a training loop:
0. Loop through data
1. Forward pass (this involves data moving through our models `forward()` function) to make predicitions on data - also called forward propogation.
2. Calculate the loss ( Complare forward pass pred to ground true lables)
3. Optimize Zero grad
4. Loss backward - moves backwards through the network to calculate the gradients of the each of the parameters of out model woth respect to the loss (**Backpropagation**)
5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (***Gradient Descent**).
"""

# An epoch is a one loop through the data...
epochs = 100

# Track different values
epoch_count= []
loss_val = []
test_loss_val = []

### Training
# 0. Loop through data
for epoch in range(epochs):
  # Set the model to training mode
  model_0.train() # train mode in PyTorch set all paramaeters that require gradient to requre gradients

  # 1. Forward pass
  Y_pred = model_0(X_train)

  # 2. Calculate the loss
  loss = loss_fn(Y_pred, Y_train) # it will give us the difference between the predicted valuse and the train values.
  # print(f"loss: {loss}")

  #3. optimizer zero grad
  optimizer.zero_grad()

  # 4. perform backpropagation on the loss with respect to the parameters of the model - Loss backward
  loss.backward()

  # 5. Optimizer step
  optimizer.step()

  ### Testing
  model_0.eval() #Truns off gradinet tracking (turn off the unused functions)
  with torch.inference_mode():
  #with torch.no_grad(): Faster with this(no grad) this is older for verson 2025

    # 1. Forward pass
    test_pred = model_0(X_test)

    # 2. Claculate the loss
    test_loss = loss_fn(test_pred, Y_test) # Y-test is the label here.

  if epoch % 10 == 0:
    epoch_count.append(epoch)
    loss_val.append(loss)
    test_loss_val.append(test_loss)
    print(f"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}")
    print(model_0.state_dict())

with torch.inference_mode():
  Y_pred_new = model_0(X_test)

plot_predicitions(predictions=Y_pred_new); # After trainings

np.array(torch.tensor(loss_val).cpu().numpy())

# plot the loss curves
plt.plot(epoch_count, np.array(torch.tensor(loss_val).numpy()), label="Train loss")
plt.plot(epoch_count, test_loss_val, label="Test loss")
plt.title("taining and testing loss curve")
plt.ylabel("loss")
plt.xlabel("epochs")
plt.legend();

"""## Saving  as model in PyTroch
There are 3 main mentho to save and load
1. `torch.save()`
2. `torch.load()`
3. `torch.nn.module.load_satae_dict()`
"""

model_0.state_dict()

from pathlib import Path
# 1. Crate a model directory
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)

# 2. Crate save path
MODEL_NAME = "01-pyTorch.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

MODEL_SAVE_PATH

# 3. SAVE THE MODEL
torch.save(obj=model_0.state_dict(),
           f=MODEL_SAVE_PATH)

!ls -1 models

"""## Loading a model"""

model_0.state_dict()

loaded_model_0 = LinearregModel()

loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

loaded_model_0.state_dict()

loaded_model_0.eval()
with torch.inference_mode():
  loaded_model_0_pred = loaded_model_0(X_test)
loaded_model_0_pred













"""### 6. Putting it all Together"""

import torch
from torch import nn
import matplotlib.pyplot as plt
torch.__version__

device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

"""### 6.1 Data"""

weight = 0.7
bias = 0.3

start = 0
end = 1
step = 0.02

x = torch.arange(start,end,step).unsqueeze(dim=1)
y = weight * x + bias
x[:10], y[:10]

# split data
train_split = int(0.8 * len(x))
x_train, y_train = x[:train_split], y[:train_split]
x_test, y_test = x[train_split:], y[train_split:]
len(x_train), len(y_train), len(x_test), len(y_test)

def plot_predicitions(train_data,
                      train_lables,
                      test_data,
                      test_lables,
                      predictions = None):

  """
  Plots training data, test data and compares predictions.
  """
  plt.figure(figsize = (10, 7))

  # Plot training data in Blue
  plt.scatter(train_data, train_lables, c="b", s=4, label="training data")

  # Plot test data in green
  plt.scatter(test_data, test_lables, c="r", s=4, label="testing data")

  # Are there predicitoons?
  if predictions is not None:
      # Plot predictions in red (predictions in test data)
      plt.scatter(test_data, predictions, c="g", s=4, label="predictions")

  # Show the lengend
  plt.legend(prop={"size": 14});

# Plot the data
plot_predicitions(x_train,y_train,x_test,y_test)

"""## 6.2 Building the model"""

# Create a linear model by subclassing nn.Module
class linerregv2(nn.Module):
  def __init__(self):
    super().__init__()

    self.layer_1 = nn.Linear(in_features=1, out_features=1)

  def forward(self, x: torch.tensor) -> torch.Tensor:
    return self.layer_1(x)

torch.manual_seed(42)
model_1 = linerregv2()

model_1, model_1.state_dict()

"""### Training

For training we need:
* Loss function
* optimizer
* Trainng loop
* Testing loop
"""

next(model_1.parameters()).device

# Set the model to use the traget device
model_1.to(device)
next(model_1.parameters()).device # if u have GPU u will get cuda

"""### 6.3 Training
For tainign we need:
* Loss function
* Optimizer
* Trainig loop
* Testing loop
"""

# Set up loss function
loss_fu = nn.L1Loss()
# Set up optimizer
optimizer = torch.optim.SGD(params=model_1.parameters(),
                            lr = 0.01)

# Training loop
torch.manual_seed(42)

epochs = 200

# Put the data on the device:
x_train = x_train.to(device)
y_train = y_train.to(device)
x_test = x_test.to(device)
y_test = y_test.to(device)

for epoch in range(epochs):
  model_1.train()
  y_pred = model_1(x_train)
  loss = loss_fn(y_pred, y_train)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  # Testing loop
  model_1.eval()
  with torch.inference_mode():
    test_pred = model_1(x_test)
    test_loss = loss_fn(test_pred, y_test)


  if epoch % 100 == 0:
    print(f"loss = {loss} | model_1.state_dict = {model_1.state_dict()} | Test loss: {test_loss}")

model_1.state_dict() # But in real life we will never know what is the real values

weight, bias

"""### 6.4 Makingand evaluating predictions"""

model_1.eval()

with torch.inference_mode():
  y_pred = model_1(x_test)
y_pred

plot_predicitions(x_train.cpu().numpy(), y_train.cpu().numpy(), x_test.cpu().numpy(), y_test.cpu().numpy(), predictions=y_pred.cpu().numpy())

model_1.state_dict()

"""### 6.5 saving and loading model"""

from pathlib import Path
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)
MODEL_NAME = "02_pyTorch.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME
MODEL_SAVE_PATH
torch.save(obj=model_1.state_dict(),
           f=MODEL_SAVE_PATH)

!ls -1 models

loaded_model_2 = linerregv2()

from pickle import load
loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))
loaded_model_2.to(device)

loaded_model_2.state_dict()

next(loaded_model_2.parameters()).device

loaded_model_2.eval()
with torch.inference_mode():
  loaded_model_2_pred = loaded_model_2(x_test)
y_pred == loaded_model_2_pred

